{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "498a8c79",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "\n",
    "Start by installing all the necessary dependencies in the virtual environment in Python with pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4d0b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U datasets transformers torchvision accelerate tensorflow-macos tensorflow-metal tf-keras numpy matplotlib torch timm scikit-learn ipykernel notebook jupyterlab gradio ipywidgets --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416c7deb",
   "metadata": {},
   "source": [
    "## 1 CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbccdc9",
   "metadata": {},
   "source": [
    "## 1.1 File paths\n",
    "\n",
    "Specify the filepaths to where the dataset, models etc should be stored.\n",
    "\n",
    "### Warning!!! Please change the following paths before running the notebooks to ensure there are no issues:\n",
    "1. cnn_model_dir \n",
    "2. vit_model_dir\n",
    "\n",
    "I changed them to be placed on an External HDD as I have not enough space. You may place them wherever you have space but ideally within the same folder like cnn_model_dir = \"./cnn-checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f970d934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Paths\n",
    "amazon_reviews_path = \"./amazon2023/All_Beauty.jsonl.gz\"\n",
    "text_model_dir = \"./bert_sentiment_best\"\n",
    "\n",
    "# CNN Paths\n",
    "cnn_model_dir = \"./cnn-checkpoints\"\n",
    "# cnn_model_dir = \"/Volumes/XuanYi's T7/cnn-checkpoints\"\n",
    "base_dir_cnn = './amazon2023-images/problem_cnn'\n",
    "\n",
    "# VIT Paths\n",
    "vit_model_dir = \"./vit-checkpoints\"\n",
    "# vit_model_dir = \"/Volumes/XuanYi's T7/vit-checkpoints\"\n",
    "base_dir_vit = './amazon2023-images/problem_vit'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641be1ab",
   "metadata": {},
   "source": [
    "# 2 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bb483ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries for Loading the Dataset and Processing the images\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c5af6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>images</th>\n",
       "      <th>asin</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>helpful_vote</th>\n",
       "      <th>verified_purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>Pretty Color</td>\n",
       "      <td>The polish was quiet thick and did not apply s...</td>\n",
       "      <td>[{'small_image_url': 'https://images-na.ssl-im...</td>\n",
       "      <td>B00R8DXL44</td>\n",
       "      <td>B00R8DXL44</td>\n",
       "      <td>AGMJ3EMDVL6OWBJF7CA5RGJLXN5A</td>\n",
       "      <td>2020-08-27 22:30:08.138</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>Meh</td>\n",
       "      <td>These were lightweight and soft but much too s...</td>\n",
       "      <td>[{'small_image_url': 'https://m.media-amazon.c...</td>\n",
       "      <td>B088SZDGXG</td>\n",
       "      <td>B08BBQ29N5</td>\n",
       "      <td>AEYORY2AVPMCPDV57CE337YU5LXA</td>\n",
       "      <td>2021-10-15 05:20:59.292</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>5</td>\n",
       "      <td>Nice colors and dries fast</td>\n",
       "      <td>Haven’t worn it very long yet so can’t comment...</td>\n",
       "      <td>[{'small_image_url': 'https://images-na.ssl-im...</td>\n",
       "      <td>B07H281V4V</td>\n",
       "      <td>B07H281V4V</td>\n",
       "      <td>AHW7W34BLHHC4AYM4TPMLA2SWMMA</td>\n",
       "      <td>2020-12-31 17:13:27.770</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>5</td>\n",
       "      <td>Absolutely beautiful</td>\n",
       "      <td>These diamond are absolutely beautiful and shi...</td>\n",
       "      <td>[{'small_image_url': 'https://m.media-amazon.c...</td>\n",
       "      <td>B095SC4J8T</td>\n",
       "      <td>B095SC4J8T</td>\n",
       "      <td>AH4CGRSYSW5CWLRGQYRZKNJBUPAA</td>\n",
       "      <td>2021-08-17 02:11:43.947</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>5</td>\n",
       "      <td>Doesn't Sting or Burn My Eyes!</td>\n",
       "      <td>I am in my late 40's and started using anti-ag...</td>\n",
       "      <td>[{'small_image_url': 'https://images-na.ssl-im...</td>\n",
       "      <td>B01CO73OIQ</td>\n",
       "      <td>B01CO73OIQ</td>\n",
       "      <td>AHV6QCNBJNSGLATP56JAWJ3C4G2A</td>\n",
       "      <td>2016-06-28 14:13:38.000</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rating                           title  \\\n",
       "5         4                    Pretty Color   \n",
       "7         3                             Meh   \n",
       "45        5      Nice colors and dries fast   \n",
       "56        5            Absolutely beautiful   \n",
       "106       5  Doesn't Sting or Burn My Eyes!   \n",
       "\n",
       "                                                  text  \\\n",
       "5    The polish was quiet thick and did not apply s...   \n",
       "7    These were lightweight and soft but much too s...   \n",
       "45   Haven’t worn it very long yet so can’t comment...   \n",
       "56   These diamond are absolutely beautiful and shi...   \n",
       "106  I am in my late 40's and started using anti-ag...   \n",
       "\n",
       "                                                images        asin  \\\n",
       "5    [{'small_image_url': 'https://images-na.ssl-im...  B00R8DXL44   \n",
       "7    [{'small_image_url': 'https://m.media-amazon.c...  B088SZDGXG   \n",
       "45   [{'small_image_url': 'https://images-na.ssl-im...  B07H281V4V   \n",
       "56   [{'small_image_url': 'https://m.media-amazon.c...  B095SC4J8T   \n",
       "106  [{'small_image_url': 'https://images-na.ssl-im...  B01CO73OIQ   \n",
       "\n",
       "    parent_asin                       user_id               timestamp  \\\n",
       "5    B00R8DXL44  AGMJ3EMDVL6OWBJF7CA5RGJLXN5A 2020-08-27 22:30:08.138   \n",
       "7    B08BBQ29N5  AEYORY2AVPMCPDV57CE337YU5LXA 2021-10-15 05:20:59.292   \n",
       "45   B07H281V4V  AHW7W34BLHHC4AYM4TPMLA2SWMMA 2020-12-31 17:13:27.770   \n",
       "56   B095SC4J8T  AH4CGRSYSW5CWLRGQYRZKNJBUPAA 2021-08-17 02:11:43.947   \n",
       "106  B01CO73OIQ  AHV6QCNBJNSGLATP56JAWJ3C4G2A 2016-06-28 14:13:38.000   \n",
       "\n",
       "     helpful_vote  verified_purchase  \n",
       "5               0               True  \n",
       "7               0               True  \n",
       "45              0               True  \n",
       "56              0               True  \n",
       "106             8               True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ingest all the Amazon Beauty Product Reviews as a dataframe\n",
    "amazon_df = pd.read_json(amazon_reviews_path, lines=True)\n",
    "\n",
    "# Filter rows with images only\n",
    "df_with_images = amazon_df[amazon_df['images'].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "df_with_images.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d089d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>not_problematic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://m.media-amazon.com/images/I/81FN4c0VHz...</td>\n",
       "      <td>uncertain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url            label\n",
       "0  https://images-na.ssl-images-amazon.com/images...  not_problematic\n",
       "1  https://m.media-amazon.com/images/I/81FN4c0VHz...        uncertain"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map the Amazon ratings provided by users to a flag for images\n",
    "def map_rating_to_label(rating):\n",
    "    if rating in [1, 2]:\n",
    "        return \"problematic\"\n",
    "    elif rating == 3:\n",
    "        return \"uncertain\"\n",
    "    else:  \n",
    "        # 4 or 5\n",
    "        return \"not_problematic\"\n",
    "\n",
    "# Flatten out image URLs and associate each with its rating\n",
    "image_rows = []\n",
    "for idx, row in df_with_images.iterrows():\n",
    "    rating = int(row['rating'])\n",
    "    label = map_rating_to_label(rating)\n",
    "\n",
    "    # Iterate through every row to extract the labels for each image and associate it with the image\n",
    "    for img_dict in row['images']:\n",
    "        url = img_dict.get('small_image_url')\n",
    "        if url:\n",
    "            image_rows.append({'url': url, 'label': label})\n",
    "\n",
    "# Convert to a Dataframe and verify the dat is what I expected\n",
    "amazon_images_df = pd.DataFrame(image_rows)\n",
    "amazon_images_df.head(n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "365f1b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image folders already exist and contain images. Skipping download.\n",
      "Number of CNN images: 25481\n",
      "Number of ViT images: 25482\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checks if the CNN & VIT folders exist and are non-empty, counts the number of images in each, and downloads images if needed, saving them in two different sizes for CNN and ViT models.\n",
    "\"\"\"\n",
    "# Check if both folders exist and are non-empty\n",
    "def folder_has_images(folder):\n",
    "    if not os.path.exists(folder):\n",
    "        return False\n",
    "    for subfolder in os.listdir(folder):\n",
    "        subfolder_path = os.path.join(folder, subfolder)\n",
    "        if os.path.isdir(subfolder_path) and len(os.listdir(subfolder_path)) > 0:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Count the number of files in each folder for verification\n",
    "def count_files_in_folder(folder):\n",
    "    total = 0\n",
    "    for subfolder in os.listdir(folder):\n",
    "        subfolder_path = os.path.join(folder, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            total += len([f for f in os.listdir(subfolder_path) if os.path.isfile(os.path.join(subfolder_path, f))])\n",
    "    return total\n",
    "\n",
    "# Check if we need to download\n",
    "if folder_has_images(base_dir_cnn) and folder_has_images(base_dir_vit):\n",
    "    print(\"Image folders already exist and contain images. Skipping download.\")\n",
    "    # Print the number of files we have\n",
    "    print(f\"Number of CNN images: {count_files_in_folder(base_dir_cnn)}\")\n",
    "    print(f\"Number of ViT images: {count_files_in_folder(base_dir_vit)}\")\n",
    "else:\n",
    "    # Empty - Download the training and test dataset\n",
    "    os.makedirs(base_dir_cnn, exist_ok=True)\n",
    "    os.makedirs(base_dir_vit, exist_ok=True)\n",
    "\n",
    "    for i, row in amazon_images_df.iterrows():\n",
    "        label = row['label']\n",
    "        url = row['url']\n",
    "\n",
    "        save_dir_cnn = f'{base_dir_cnn}/{label}'\n",
    "        save_dir_vit = f'{base_dir_vit}/{label}'\n",
    "\n",
    "        os.makedirs(save_dir_cnn, exist_ok=True)\n",
    "        os.makedirs(save_dir_vit, exist_ok=True)\n",
    "\n",
    "        # Check if image already exists before downloading\n",
    "        cnn_img_path = f'{save_dir_cnn}/{i}.jpg'\n",
    "        vit_img_path = f'{save_dir_vit}/{i}.jpg'\n",
    "        if os.path.exists(cnn_img_path) and os.path.exists(vit_img_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "                img_cnn = img.resize((128, 128))\n",
    "                img_cnn.save(cnn_img_path)\n",
    "                img_vit = img.resize((224, 224))\n",
    "                img_vit.save(vit_img_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6846ad25",
   "metadata": {},
   "source": [
    "**Load images for CNN Training first**\n",
    "\n",
    "- DataGenerator will help to load, preprocess and augment images as model trains like rescaling, rotation to increase diversity of dataset without increasing the number of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e946eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Libraries\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64c55244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20386 images belonging to 3 classes.\n",
      "Found 5095 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Creates a Keras ImageDataGenerator to preprocess and augment image data if necessary for CNN, with 20% as test dataset\n",
    "cnn_data_generator = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "# Loads dataset in batches\n",
    "# Loads images from a folder structure where each subfolder represents a class label\n",
    "train_cnn_data_generator = cnn_data_generator.flow_from_directory(\n",
    "    base_dir_cnn,\n",
    "    # Resize the image\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    # Use 80-% of dataset for training\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Same as above, but for testing purposes\n",
    "test_cnn_data_generator = cnn_data_generator.flow_from_directory(\n",
    "    base_dir_cnn,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "632d8409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-18 16:17:39.140109: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-10-18 16:17:39.140321: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-10-18 16:17:39.140338: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-10-18 16:17:39.140680: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-10-18 16:17:39.140702: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# CNN Architecture - Using a Pre-Trained CNN ResNet which is used for Image Classification\n",
    "# Remove the top (i.e. original classification head) to classify the pre-trained classes\n",
    "base_cnn_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "# Freeze base model for initial training\n",
    "base_cnn_model.trainable = False  \n",
    "\n",
    "# Add my own classification head as the last year to predict 1 of 3 labels\n",
    "cnn_model = Sequential([\n",
    "    base_cnn_model,\n",
    "    # Converts output of base model into 1D vector\n",
    "    Flatten(),\n",
    "    # For learning patterns\n",
    "    Dense(128, activation='relu'),\n",
    "    # Prevent overfitting by setting 50% to 0\n",
    "    Dropout(0.5),\n",
    "    # Final layer for classification\n",
    "    Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "333d7ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computes class weights in case dataset is imbalanced since Amazon usually is biased, a lot of bad or good reviews\n",
    "\"\"\"\n",
    "# Maps classnames to integer labels\n",
    "class_indices = train_cnn_data_generator.class_indices\n",
    "classes = list(class_indices.values())\n",
    "\n",
    "# Get all int labels\n",
    "labels = train_cnn_data_generator.classes\n",
    "\n",
    "# Maps class weight -> label\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights_dict = dict(zip(np.unique(labels), class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46a4c963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngxua/Documents/ComputerScience_Projects/BC3415-AI-in-Finance/BC3415-Individual-Assignment/tfmacenv/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "2025-10-18 16:17:42.717016: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.3474 - loss: 48.9339\n",
      "Epoch 1: val_loss improved from None to 2.97391, saving model to /Volumes/XuanYi's T7/cnn-checkpoints/cnn_best_trained_model.keras\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 216ms/step - accuracy: 0.3361 - loss: 37.6445 - val_accuracy: 0.1666 - val_loss: 2.9739\n",
      "Epoch 2/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.3332 - loss: 7.0114\n",
      "Epoch 2: val_loss did not improve from 2.97391\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 202ms/step - accuracy: 0.3274 - loss: 5.0060 - val_accuracy: 0.0795 - val_loss: 4.2597\n",
      "Epoch 3/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.3285 - loss: 2.7062\n",
      "Epoch 3: val_loss improved from 2.97391 to 0.81911, saving model to /Volumes/XuanYi's T7/cnn-checkpoints/cnn_best_trained_model.keras\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 220ms/step - accuracy: 0.3307 - loss: 2.6259 - val_accuracy: 0.7827 - val_loss: 0.8191\n",
      "Epoch 4/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.3400 - loss: 2.5353\n",
      "Epoch 4: val_loss improved from 0.81911 to 0.73515, saving model to /Volumes/XuanYi's T7/cnn-checkpoints/cnn_best_trained_model.keras\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 223ms/step - accuracy: 0.3360 - loss: 2.4871 - val_accuracy: 0.7827 - val_loss: 0.7351\n",
      "Epoch 5/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.3252 - loss: 2.2764\n",
      "Epoch 5: val_loss did not improve from 0.73515\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 209ms/step - accuracy: 0.3285 - loss: 2.2814 - val_accuracy: 0.1376 - val_loss: 2.1364\n",
      "Epoch 6/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.3330 - loss: 2.3043\n",
      "Epoch 6: val_loss did not improve from 0.73515\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 197ms/step - accuracy: 0.3345 - loss: 2.3569 - val_accuracy: 0.7827 - val_loss: 0.8661\n",
      "Epoch 7/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.3211 - loss: 2.4102\n",
      "Epoch 7: val_loss did not improve from 0.73515\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 201ms/step - accuracy: 0.3260 - loss: 2.3980 - val_accuracy: 0.7806 - val_loss: 0.8991\n",
      "Epoch 8/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.3346 - loss: 2.5838\n",
      "Epoch 8: val_loss did not improve from 0.73515\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 201ms/step - accuracy: 0.3345 - loss: 2.7089 - val_accuracy: 0.1372 - val_loss: 1.6240\n",
      "Epoch 9/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.3242 - loss: 2.1950\n",
      "Epoch 9: val_loss did not improve from 0.73515\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 200ms/step - accuracy: 0.3214 - loss: 2.2578 - val_accuracy: 0.0797 - val_loss: 2.1401\n",
      "Epoch 10/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.3278 - loss: 2.6685\n",
      "Epoch 10: val_loss did not improve from 0.73515\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 206ms/step - accuracy: 0.3297 - loss: 2.9094 - val_accuracy: 0.1376 - val_loss: 1.3072\n",
      "Epoch 11/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.3249 - loss: 2.4225\n",
      "Epoch 11: val_loss did not improve from 0.73515\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 202ms/step - accuracy: 0.3274 - loss: 2.5112 - val_accuracy: 0.7827 - val_loss: 0.7617\n",
      "Epoch 12/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.3284 - loss: 2.4216\n",
      "Epoch 12: val_loss did not improve from 0.73515\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 203ms/step - accuracy: 0.3321 - loss: 2.4223 - val_accuracy: 0.7827 - val_loss: 0.9404\n",
      "Epoch 13/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.3421 - loss: 2.4833\n",
      "Epoch 13: val_loss did not improve from 0.73515\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 204ms/step - accuracy: 0.3340 - loss: 2.3868 - val_accuracy: 0.0797 - val_loss: 1.7148\n",
      "Epoch 14/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.3390 - loss: 2.4397\n",
      "Epoch 14: val_loss did not improve from 0.73515\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 203ms/step - accuracy: 0.3347 - loss: 2.4943 - val_accuracy: 0.0797 - val_loss: 3.7067\n",
      "Epoch 15/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.3230 - loss: 2.2864\n",
      "Epoch 15: val_loss did not improve from 0.73515\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 203ms/step - accuracy: 0.3358 - loss: 2.4926 - val_accuracy: 0.0797 - val_loss: 1.9578\n",
      "Epoch 16/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.3205 - loss: 2.5083\n",
      "Epoch 16: val_loss did not improve from 0.73515\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 203ms/step - accuracy: 0.3241 - loss: 2.3357 - val_accuracy: 0.1374 - val_loss: 3.1426\n",
      "Epoch 17/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.3323 - loss: 2.5244\n",
      "Epoch 17: val_loss did not improve from 0.73515\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 199ms/step - accuracy: 0.3339 - loss: 2.4804 - val_accuracy: 0.1376 - val_loss: 3.9272\n",
      "Epoch 18/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.3345 - loss: 2.5991\n",
      "Epoch 18: val_loss did not improve from 0.73515\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 202ms/step - accuracy: 0.3332 - loss: 2.5061 - val_accuracy: 0.1058 - val_loss: 1.0846\n",
      "Epoch 19/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.3194 - loss: 2.4107\n",
      "Epoch 19: val_loss did not improve from 0.73515\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 201ms/step - accuracy: 0.3241 - loss: 2.3695 - val_accuracy: 0.0797 - val_loss: 1.8344\n",
      "Epoch 20/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.3451 - loss: 2.6476\n",
      "Epoch 20: val_loss did not improve from 0.73515\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 199ms/step - accuracy: 0.3376 - loss: 2.5096 - val_accuracy: 0.7827 - val_loss: 0.7961\n"
     ]
    }
   ],
   "source": [
    "# Ensure the best model trained is saved and not the most recent one trained\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=f\"{cnn_model_dir}/cnn_best_trained_model.keras\",\n",
    "    # Checks validation loss metric, which is usaed to determine beest model\n",
    "    monitor='val_loss', \n",
    "    # Save model iif val loss improves\n",
    "    save_best_only=True, \n",
    "    # Save entire model\n",
    "    save_weights_only=False,\n",
    "    # lower val loss is better ofc\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Use AdamW optimiser for ML, and tracks accuracy for training and validation\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with generators for 20 epochs, i.e. see data 20 times\n",
    "cnn_model_training_history = cnn_model.fit(\n",
    "    train_cnn_data_generator,\n",
    "    validation_data=test_cnn_data_generator,\n",
    "    epochs=20,\n",
    "    # Adjust loss function for imbalanced classes\n",
    "    class_weight=class_weights_dict,\n",
    "    # Save best model only\n",
    "    callbacks=[checkpoint],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43e5aed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 161ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88      3988\n",
      "           1       0.00      0.00      0.00       701\n",
      "           2       0.00      0.00      0.00       406\n",
      "\n",
      "    accuracy                           0.78      5095\n",
      "   macro avg       0.26      0.33      0.29      5095\n",
      "weighted avg       0.61      0.78      0.69      5095\n",
      "\n",
      "[[3988    0    0]\n",
      " [ 701    0    0]\n",
      " [ 406    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# Reset test datagenerator so we can test from first batch\n",
    "test_cnn_data_generator.reset()\n",
    "\n",
    "# Get the predicted probs for each class\n",
    "cnn_preds = cnn_model.predict(test_cnn_data_generator)\n",
    "# Cast the prob into their labels\n",
    "cnn_pred_labels = np.argmax(cnn_preds, axis=1)\n",
    "cnn_true_labels = test_cnn_data_generator.classes\n",
    "\n",
    "# Prints metrics and scores of our test so we can evaluate\n",
    "print(classification_report(cnn_true_labels, cnn_pred_labels, zero_division=0))\n",
    "print(confusion_matrix(cnn_true_labels, cnn_pred_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e4b86c",
   "metadata": {},
   "source": [
    "# VIT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f1ccbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIT Libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoImageProcessor\n",
    "from transformers import AutoModelForImageClassification\n",
    "from transformers import DefaultDataCollator\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7d80038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f62a758530f46fcafb12a835e2ceefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Uses HF load_dataset and imagefolder builder to load images from vit images where each subfolder is a label\n",
    "vit_dataset = load_dataset(\n",
    "    \"imagefolder\",\n",
    "    data_dir=base_dir_vit,\n",
    ")\n",
    "\n",
    "# Split into train and test\n",
    "vit_dataset = vit_dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "train_vit_dataset = vit_dataset[\"train\"]\n",
    "test_vit_dataset = vit_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1eeae328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d146e9ad3048b58d24d52ac9539c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ef270f3a0c4d14bef93c3f55752ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25779ec82e864b19a096b20a94a9b04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997c874461dd442d82f6ca55ffa39672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5097 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loads a pre-trained processor who can resize, normalize, and patchify images for VIT training\n",
    "vit_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "# Preprocess a batch of images into Tensors for both train and test for VIT\n",
    "def preprocess_vit_tensors(batch):\n",
    "    inputs = vit_processor(batch[\"image\"], return_tensors=\"pt\")\n",
    "    batch[\"pixel_values\"] = inputs[\"pixel_values\"]\n",
    "    return batch\n",
    "\n",
    "# Apply to both train and test\n",
    "train_vit_dataset = train_vit_dataset.map(preprocess_vit_tensors, batched=True)\n",
    "test_vit_dataset = test_vit_dataset.map(preprocess_vit_tensors, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e4e5cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae09c3574fa4160abb17a64b2dcb532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Maps class labels for beauty products to ints for training\n",
    "label_to_id = {\n",
    "    \"problematic\": 0,\n",
    "    \"uncertain\": 1,\n",
    "    \"not_problematic\": 2,\n",
    "}\n",
    "# Reverse mapping so we can get back our mappings\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "\n",
    "# Loads a pre-trained VIT model to reduce the training necessary\n",
    "vit_model = AutoModelForImageClassification.from_pretrained(\n",
    "    # Takes pre-trained-model\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    # Only problem, no problem and uncertain\n",
    "    num_labels=3,\n",
    "    # Ensure model outputs are mapped to my labels for prediction\n",
    "    label2id=label_to_id,\n",
    "    id2label=id_to_label,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42498b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batches and formatting of data during training\n",
    "vit_collator = DefaultDataCollator()\n",
    "\n",
    "# Arguments for Training\n",
    "vit_training_args = TrainingArguments(\n",
    "    # Save to specified location, my external ssd as laptop no space\n",
    "    output_dir=vit_model_dir,\n",
    "    # Batch size for gpu and cpu\n",
    "    per_device_train_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    # Log progress every 50 steps\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    # Reload the model with lowest val loss after train\n",
    "    load_best_model_at_end=True,\n",
    "    # Use val loss to determine best model\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aace9de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngxua/Documents/ComputerScience_Projects/BC3415-AI-in-Finance/BC3415-Individual-Assignment/tfmacenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6370' max='6370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6370/6370 8:01:11, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.626200</td>\n",
       "      <td>0.645599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.434800</td>\n",
       "      <td>0.665912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.134500</td>\n",
       "      <td>0.946515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>1.390630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.520901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.623971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.692511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.727897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.753140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.763966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngxua/Documents/ComputerScience_Projects/BC3415-AI-in-Finance/BC3415-Individual-Assignment/tfmacenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/BC3415-AI-in-Finance/BC3415-Individual-Assignment/tfmacenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/BC3415-AI-in-Finance/BC3415-Individual-Assignment/tfmacenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/BC3415-AI-in-Finance/BC3415-Individual-Assignment/tfmacenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/BC3415-AI-in-Finance/BC3415-Individual-Assignment/tfmacenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/BC3415-AI-in-Finance/BC3415-Individual-Assignment/tfmacenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/BC3415-AI-in-Finance/BC3415-Individual-Assignment/tfmacenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/BC3415-AI-in-Finance/BC3415-Individual-Assignment/tfmacenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/BC3415-AI-in-Finance/BC3415-Individual-Assignment/tfmacenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6370, training_loss=0.11906840492442347, metrics={'train_runtime': 28877.1377, 'train_samples_per_second': 7.059, 'train_steps_per_second': 0.221, 'total_flos': 1.579610873729581e+19, 'train_loss': 0.11906840492442347, 'epoch': 10.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a trainer based on training params and actually train\n",
    "trainer = Trainer(\n",
    "    model=vit_model,\n",
    "    args=vit_training_args,\n",
    "    train_dataset=train_vit_dataset,\n",
    "    eval_dataset=test_vit_dataset,\n",
    "    data_collator=vit_collator,\n",
    "    processing_class=vit_processor\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567805a5",
   "metadata": {},
   "source": [
    "# Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43610772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploying model libraries\n",
    "import gradio as gr\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForImageClassification, AutoImageProcessor\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bcd0c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper Function to flag problematic orders based on textual review and product images\n",
    "\"\"\"\n",
    "def flag_order(text, img):\n",
    "    # Sentiment prediction\n",
    "    sentiment_res = sentiment_clf(text)[0]\n",
    "    sentiment_label = sentiment_res['label'].lower()\n",
    "    sentiment_output = f\"{sentiment_label.capitalize()} ({sentiment_res['score']:.3f})\"\n",
    "    \n",
    "    # Image prediction - optional\n",
    "    if img is not None:\n",
    "        inputs = image_processor(img, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = image_model(**inputs)\n",
    "            image_pred = outputs.logits.argmax(-1).item()\n",
    "        image_label = id_to_label.get(image_pred, f\"Class {image_pred}\")\n",
    "    else:\n",
    "        image_label = \"No image provided\"\n",
    "    \n",
    "    # Determine if order is problematic with new logic\n",
    "    if sentiment_label == \"negative\":\n",
    "        overall_flag = \"problematic\"\n",
    "    elif sentiment_label == \"neutral\" and image_label == \"problematic\":\n",
    "        overall_flag = \"problematic\"\n",
    "    else:\n",
    "        overall_flag = \"not_problematic\"\n",
    "    \n",
    "    return sentiment_output, image_label, overall_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2b8d314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the Best VIT and Text Models\n",
    "image_checkpoint_dir = f\"{vit_model_dir}/checkpoint-6370\"\n",
    "\n",
    "# Load sentiment analysis pipeline to predict sentiment of review\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(text_model_dir)\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(text_model_dir)\n",
    "sentiment_clf = pipeline(\"text-classification\", model=sentiment_model, tokenizer=sentiment_tokenizer)\n",
    "\n",
    "def predict_sentiment(text: str):\n",
    "    res = sentiment_clf(text)[0]\n",
    "    return f\"{res['label']} ({res['score']:.3f})\"\n",
    "\n",
    "# Load image classification model\n",
    "image_model = AutoModelForImageClassification.from_pretrained(image_checkpoint_dir)\n",
    "image_processor = AutoImageProcessor.from_pretrained(image_checkpoint_dir)\n",
    "\n",
    "def predict_image(img):\n",
    "    inputs = image_processor(img, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = image_model(**inputs)\n",
    "        pred = outputs.logits.argmax(-1).item()\n",
    "    return f\"Predicted class: {pred}\"\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=flag_order,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=3, placeholder=\"Enter a review...\"),\n",
    "        gr.Image(type=\"pil\", label=\"Order Image (optional)\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Sentiment\"),\n",
    "        gr.Textbox(label=\"Image Classification\"),\n",
    "        gr.Textbox(label=\"Overall Flag\")\n",
    "    ],\n",
    "    title=\"Order Review & Image Analysis\",\n",
    "    description=\"Enter a review and optionally upload an order image to get combined insights.\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076a8c3e",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "- Not Problematic images are diverse, while problematic and uncertain images are usually the same, i.e. we follow text review more. If text is positive, but picture is damaged box, review overall should be positive\n",
    "- Similarly, images are useful when text is uncertain, then we follow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfmacenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
