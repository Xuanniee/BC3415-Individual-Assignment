{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de2a17e",
   "metadata": {},
   "source": [
    "# BC3415 Individual Assignment: Real-World Applications of Text and Image Classification\n",
    "\n",
    "Requirements:\n",
    "\n",
    "A major online retail company receives thousands of product reviews daily. These reviews often contain both text comments and customer-uploaded product images. The company wants to:\n",
    "- Classify text reviews into sentiment categories (positive, negative, neutral)\n",
    "- Detect product defects or misdeliveries from customer-uploaded images\n",
    "- Combine both types of information to automatically flag problematic orders (More of prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6bcb4a",
   "metadata": {},
   "source": [
    "## 1 Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41fcb523",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers datasets accelerate evaluate scikit-learn scikit-learn pandas numpy gradio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605737a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "# SciPy\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Hugging Face Transformers & Datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import Dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc2c0e0",
   "metadata": {},
   "source": [
    "## 2 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ea9d30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>images</th>\n",
       "      <th>asin</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>helpful_vote</th>\n",
       "      <th>verified_purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Such a lovely scent but not overpowering.</td>\n",
       "      <td>This spray is really nice. It smells really go...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B00YQ6X8EO</td>\n",
       "      <td>B00YQ6X8EO</td>\n",
       "      <td>AGKHLEW2SOWHNMFQIJGBECAF7INQ</td>\n",
       "      <td>2020-05-05 14:08:48.923</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Works great but smells a little weird.</td>\n",
       "      <td>This product does what I need it to do, I just...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B081TJ8YS3</td>\n",
       "      <td>B081TJ8YS3</td>\n",
       "      <td>AGKHLEW2SOWHNMFQIJGBECAF7INQ</td>\n",
       "      <td>2020-05-04 18:10:55.070</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Yes!</td>\n",
       "      <td>Smells good, feels great!</td>\n",
       "      <td>[]</td>\n",
       "      <td>B07PNNCSP9</td>\n",
       "      <td>B097R46CSY</td>\n",
       "      <td>AE74DYR3QUGVPZJ3P7RFWBGIX7XQ</td>\n",
       "      <td>2020-05-16 21:41:06.052</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Synthetic feeling</td>\n",
       "      <td>Felt synthetic</td>\n",
       "      <td>[]</td>\n",
       "      <td>B09JS339BZ</td>\n",
       "      <td>B09JS339BZ</td>\n",
       "      <td>AFQLNQNQYFWQZPJQZS6V3NZU4QBQ</td>\n",
       "      <td>2022-01-28 18:13:50.220</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>A+</td>\n",
       "      <td>Love it</td>\n",
       "      <td>[]</td>\n",
       "      <td>B08BZ63GMJ</td>\n",
       "      <td>B08BZ63GMJ</td>\n",
       "      <td>AFQLNQNQYFWQZPJQZS6V3NZU4QBQ</td>\n",
       "      <td>2020-12-30 10:02:43.534</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>Pretty Color</td>\n",
       "      <td>The polish was quiet thick and did not apply s...</td>\n",
       "      <td>[{'small_image_url': 'https://images-na.ssl-im...</td>\n",
       "      <td>B00R8DXL44</td>\n",
       "      <td>B00R8DXL44</td>\n",
       "      <td>AGMJ3EMDVL6OWBJF7CA5RGJLXN5A</td>\n",
       "      <td>2020-08-27 22:30:08.138</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>Handy</td>\n",
       "      <td>Great for many tasks.  I purchased these for m...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B099DRHW5V</td>\n",
       "      <td>B099DRHW5V</td>\n",
       "      <td>AHREXOGQPZDA6354MHH4ETSF3MCQ</td>\n",
       "      <td>2021-09-17 13:31:59.443</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>Meh</td>\n",
       "      <td>These were lightweight and soft but much too s...</td>\n",
       "      <td>[{'small_image_url': 'https://m.media-amazon.c...</td>\n",
       "      <td>B088SZDGXG</td>\n",
       "      <td>B08BBQ29N5</td>\n",
       "      <td>AEYORY2AVPMCPDV57CE337YU5LXA</td>\n",
       "      <td>2021-10-15 05:20:59.292</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>Great for at home use and so easy to use!</td>\n",
       "      <td>This is perfect for my between salon visits. I...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B08P2DZB4X</td>\n",
       "      <td>B08P2DZB4X</td>\n",
       "      <td>AFSKPY37N3C43SOI5IEXEK5JSIYA</td>\n",
       "      <td>2021-07-27 13:04:04.559</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>Nice shampoo for the money</td>\n",
       "      <td>I get Keratin treatments at the salon at least...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B086QY6T7N</td>\n",
       "      <td>B086QY6T7N</td>\n",
       "      <td>AFSKPY37N3C43SOI5IEXEK5JSIYA</td>\n",
       "      <td>2021-07-18 13:21:51.145</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                      title  \\\n",
       "0       5  Such a lovely scent but not overpowering.   \n",
       "1       4     Works great but smells a little weird.   \n",
       "2       5                                       Yes!   \n",
       "3       1                          Synthetic feeling   \n",
       "4       5                                         A+   \n",
       "5       4                               Pretty Color   \n",
       "6       5                                      Handy   \n",
       "7       3                                        Meh   \n",
       "8       5  Great for at home use and so easy to use!   \n",
       "9       5                 Nice shampoo for the money   \n",
       "\n",
       "                                                text  \\\n",
       "0  This spray is really nice. It smells really go...   \n",
       "1  This product does what I need it to do, I just...   \n",
       "2                          Smells good, feels great!   \n",
       "3                                     Felt synthetic   \n",
       "4                                            Love it   \n",
       "5  The polish was quiet thick and did not apply s...   \n",
       "6  Great for many tasks.  I purchased these for m...   \n",
       "7  These were lightweight and soft but much too s...   \n",
       "8  This is perfect for my between salon visits. I...   \n",
       "9  I get Keratin treatments at the salon at least...   \n",
       "\n",
       "                                              images        asin parent_asin  \\\n",
       "0                                                 []  B00YQ6X8EO  B00YQ6X8EO   \n",
       "1                                                 []  B081TJ8YS3  B081TJ8YS3   \n",
       "2                                                 []  B07PNNCSP9  B097R46CSY   \n",
       "3                                                 []  B09JS339BZ  B09JS339BZ   \n",
       "4                                                 []  B08BZ63GMJ  B08BZ63GMJ   \n",
       "5  [{'small_image_url': 'https://images-na.ssl-im...  B00R8DXL44  B00R8DXL44   \n",
       "6                                                 []  B099DRHW5V  B099DRHW5V   \n",
       "7  [{'small_image_url': 'https://m.media-amazon.c...  B088SZDGXG  B08BBQ29N5   \n",
       "8                                                 []  B08P2DZB4X  B08P2DZB4X   \n",
       "9                                                 []  B086QY6T7N  B086QY6T7N   \n",
       "\n",
       "                        user_id               timestamp  helpful_vote  \\\n",
       "0  AGKHLEW2SOWHNMFQIJGBECAF7INQ 2020-05-05 14:08:48.923             0   \n",
       "1  AGKHLEW2SOWHNMFQIJGBECAF7INQ 2020-05-04 18:10:55.070             1   \n",
       "2  AE74DYR3QUGVPZJ3P7RFWBGIX7XQ 2020-05-16 21:41:06.052             2   \n",
       "3  AFQLNQNQYFWQZPJQZS6V3NZU4QBQ 2022-01-28 18:13:50.220             0   \n",
       "4  AFQLNQNQYFWQZPJQZS6V3NZU4QBQ 2020-12-30 10:02:43.534             0   \n",
       "5  AGMJ3EMDVL6OWBJF7CA5RGJLXN5A 2020-08-27 22:30:08.138             0   \n",
       "6  AHREXOGQPZDA6354MHH4ETSF3MCQ 2021-09-17 13:31:59.443             0   \n",
       "7  AEYORY2AVPMCPDV57CE337YU5LXA 2021-10-15 05:20:59.292             0   \n",
       "8  AFSKPY37N3C43SOI5IEXEK5JSIYA 2021-07-27 13:04:04.559             0   \n",
       "9  AFSKPY37N3C43SOI5IEXEK5JSIYA 2021-07-18 13:21:51.145             0   \n",
       "\n",
       "   verified_purchase  \n",
       "0               True  \n",
       "1               True  \n",
       "2               True  \n",
       "3               True  \n",
       "4               True  \n",
       "5               True  \n",
       "6               True  \n",
       "7               True  \n",
       "8              False  \n",
       "9              False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ingest all the Amazon Beauty Product Reviews as a dataframe\n",
    "amazon_reviews_path = \"./amazon2023/All_Beauty.jsonl.gz\"\n",
    "amazon_df = pd.read_json(amazon_reviews_path, lines=True)\n",
    "\n",
    "# Print to see\n",
    "amazon_df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988ba39f",
   "metadata": {},
   "source": [
    "## 3 Data Cleaning & Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffad17c",
   "metadata": {},
   "source": [
    "### 3.1 Static Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bdb389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an additional column where review title and body are concatenated together\n",
    "amazon_df[\"concatenated_review\"] = (\n",
    "    amazon_df[\"title\"].fillna(\"\").astype(str).str.strip() + \"\\n\" +\n",
    "    amazon_df[\"text\"].fillna(\"\").astype(str).str.strip()\n",
    ").str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcbc8d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Raw Dataset Results ########\n",
      "Number of Rows: 701528\n",
      "Number of Columns: 11\n",
      "Number of Cells: 7716808\n",
      "Dataset Columns: rating title text images asin parent_asin user_id timestamp helpful_vote verified_purchase concatenated_review \n",
      "\n",
      "######## Duplicate Check for No Image Dataset Results ########\n",
      "Exact duplicate rows: 7275\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in the dataframe columns (except for images as not hashable)\n",
    "cols_to_exclude = [\"images\"]\n",
    "no_images_df = amazon_df.drop(columns=[c for c in cols_to_exclude if c in amazon_df.columns]).copy()\n",
    "\n",
    "# View the shape of the Dataset\n",
    "num_rows, num_cols = amazon_df.shape\n",
    "num_cells = amazon_df.size\n",
    "amazon_df_cols = list(amazon_df.columns)\n",
    "print(\"######## Raw Dataset Results ########\")\n",
    "print(f\"Number of Rows: {num_rows}\")\n",
    "print(f\"Number of Columns: {num_cols}\")\n",
    "print(f\"Number of Cells: {num_cells}\")\n",
    "print(\"Dataset Columns: \", end=\"\")\n",
    "for col in amazon_df_cols:\n",
    "    print(col, end=\" \")\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"######## Duplicate Check for No Image Dataset Results ########\")\n",
    "row_dup_count = no_images_df.duplicated(keep=\"first\").sum()\n",
    "print(\"Exact duplicate rows:\", row_dup_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaa4cae",
   "metadata": {},
   "source": [
    "### 3.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc921b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Missing</th>\n",
       "      <th>Missing_%</th>\n",
       "      <th>Dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rating</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>title</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>images</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>asin</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>parent_asin</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>user_id</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>timestamp</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>helpful_vote</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>verified_purchase</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>concatenated_review</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Column  Missing  Missing_%           Dtype\n",
       "0                rating        0        0.0           int64\n",
       "1                 title        0        0.0          object\n",
       "2                  text        0        0.0          object\n",
       "3                images        0        0.0          object\n",
       "4                  asin        0        0.0          object\n",
       "5           parent_asin        0        0.0          object\n",
       "6               user_id        0        0.0          object\n",
       "7             timestamp        0        0.0  datetime64[ns]\n",
       "8          helpful_vote        0        0.0           int64\n",
       "9     verified_purchase        0        0.0            bool\n",
       "10  concatenated_review        0        0.0          object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper function to check for null and missing values\n",
    "def null_summary(df: pd.DataFrame, sort_by=\"Missing\", ascending=False, pct_round=2):\n",
    "    counts = df.isnull().sum()\n",
    "    pct = (counts / len(df) * 100).round(pct_round)\n",
    "    out = pd.DataFrame({\n",
    "        \"Column\": counts.index,\n",
    "        \"Missing\": counts.values,\n",
    "        \"Missing_%\": pct.values,\n",
    "        \"Dtype\": df.dtypes.astype(str).values\n",
    "    }).sort_values(sort_by, ascending=ascending, kind=\"mergesort\").reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "# Print to see the current state of the dataset\n",
    "summary = null_summary(amazon_df)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30fa08b",
   "metadata": {},
   "source": [
    "From the data types above, we note that the following fields needs additional formatting & cleaning:\n",
    "- title, text, asin, parent_asin, user_id should be casted to strings\n",
    "- verified_purchase should be converted to numeric values using one-hot encoding to be useful\n",
    "- images appears to be a list, so it can be left as an object for now\n",
    "\n",
    "Additionally, we notice that there are a lot of duplicate reviews but we do not remove them as it is possible for the same product to provide a similar if not identical experience for customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b80c9ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to help encode the boolean verified purchase into integers\n",
    "def encode_column(df, target_col):\n",
    "    df = df.copy()\n",
    "    if target_col not in df.columns:\n",
    "        return df\n",
    "    \n",
    "    # Normalize to boolean/nullable boolean first\n",
    "    df[target_col] = df[target_col].astype(\"boolean\")\n",
    "    # single 0/1 column where True->1, False/NA->0\n",
    "    df[target_col + \"_int\"] = df[target_col].fillna(False).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Encode Verified Purchase from Bool to Int\n",
    "amazon_df = encode_column(amazon_df, \"verified_purchase\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b214992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Missing</th>\n",
       "      <th>Missing_%</th>\n",
       "      <th>Dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rating</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>title</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>images</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>asin</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>parent_asin</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>user_id</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>timestamp</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>helpful_vote</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>verified_purchase</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>boolean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>concatenated_review</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>verified_purchase_int</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Column  Missing  Missing_%           Dtype\n",
       "0                  rating        0        0.0           int64\n",
       "1                   title        0        0.0          string\n",
       "2                    text        0        0.0          string\n",
       "3                  images        0        0.0          object\n",
       "4                    asin        0        0.0          string\n",
       "5             parent_asin        0        0.0          string\n",
       "6                 user_id        0        0.0          string\n",
       "7               timestamp        0        0.0  datetime64[ns]\n",
       "8            helpful_vote        0        0.0           int64\n",
       "9       verified_purchase        0        0.0         boolean\n",
       "10    concatenated_review        0        0.0          string\n",
       "11  verified_purchase_int        0        0.0           int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Type cast the columns to the correct dtype\n",
    "string_cols = ['title', 'text', 'asin', 'parent_asin', 'user_id', 'concatenated_review']\n",
    "for field in string_cols:\n",
    "    amazon_df[field] = amazon_df[field].astype(\"string\")\n",
    "\n",
    "# Verify results\n",
    "summary = null_summary(amazon_df)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ac1325b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>helpful_vote</th>\n",
       "      <th>verified_purchase_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>701528.000000</td>\n",
       "      <td>701528</td>\n",
       "      <td>701528.000000</td>\n",
       "      <td>701528.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.960245</td>\n",
       "      <td>2019-04-09 03:31:48.115045888</td>\n",
       "      <td>0.923588</td>\n",
       "      <td>0.905123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2000-11-01 04:24:18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2017-08-01 19:39:25.777499904</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>2019-10-20 18:11:28.616499968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>2021-03-02 01:05:05.557999872</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>2023-09-09 00:39:36.666000</td>\n",
       "      <td>646.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.494452</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.471391</td>\n",
       "      <td>0.293045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              rating                      timestamp   helpful_vote  \\\n",
       "count  701528.000000                         701528  701528.000000   \n",
       "mean        3.960245  2019-04-09 03:31:48.115045888       0.923588   \n",
       "min         1.000000            2000-11-01 04:24:18       0.000000   \n",
       "25%         3.000000  2017-08-01 19:39:25.777499904       0.000000   \n",
       "50%         5.000000  2019-10-20 18:11:28.616499968       0.000000   \n",
       "75%         5.000000  2021-03-02 01:05:05.557999872       1.000000   \n",
       "max         5.000000     2023-09-09 00:39:36.666000     646.000000   \n",
       "std         1.494452                            NaN       5.471391   \n",
       "\n",
       "       verified_purchase_int  \n",
       "count          701528.000000  \n",
       "mean                0.905123  \n",
       "min                 0.000000  \n",
       "25%                 1.000000  \n",
       "50%                 1.000000  \n",
       "75%                 1.000000  \n",
       "max                 1.000000  \n",
       "std                 0.293045  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Static Analysis of Numeric Cols\n",
    "amazon_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0278056d",
   "metadata": {},
   "source": [
    "### 3.2 Creating Labels for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5977246d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>images</th>\n",
       "      <th>asin</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>helpful_vote</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>concatenated_review</th>\n",
       "      <th>verified_purchase_int</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Such a lovely scent but not overpowering.</td>\n",
       "      <td>This spray is really nice. It smells really go...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B00YQ6X8EO</td>\n",
       "      <td>B00YQ6X8EO</td>\n",
       "      <td>AGKHLEW2SOWHNMFQIJGBECAF7INQ</td>\n",
       "      <td>2020-05-05 14:08:48.923</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Such a lovely scent but not overpowering.\\nThi...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Works great but smells a little weird.</td>\n",
       "      <td>This product does what I need it to do, I just...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B081TJ8YS3</td>\n",
       "      <td>B081TJ8YS3</td>\n",
       "      <td>AGKHLEW2SOWHNMFQIJGBECAF7INQ</td>\n",
       "      <td>2020-05-04 18:10:55.070</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>Works great but smells a little weird.\\nThis p...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Yes!</td>\n",
       "      <td>Smells good, feels great!</td>\n",
       "      <td>[]</td>\n",
       "      <td>B07PNNCSP9</td>\n",
       "      <td>B097R46CSY</td>\n",
       "      <td>AE74DYR3QUGVPZJ3P7RFWBGIX7XQ</td>\n",
       "      <td>2020-05-16 21:41:06.052</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>Yes!\\nSmells good, feels great!</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Synthetic feeling</td>\n",
       "      <td>Felt synthetic</td>\n",
       "      <td>[]</td>\n",
       "      <td>B09JS339BZ</td>\n",
       "      <td>B09JS339BZ</td>\n",
       "      <td>AFQLNQNQYFWQZPJQZS6V3NZU4QBQ</td>\n",
       "      <td>2022-01-28 18:13:50.220</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Synthetic feeling\\nFelt synthetic</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>A+</td>\n",
       "      <td>Love it</td>\n",
       "      <td>[]</td>\n",
       "      <td>B08BZ63GMJ</td>\n",
       "      <td>B08BZ63GMJ</td>\n",
       "      <td>AFQLNQNQYFWQZPJQZS6V3NZU4QBQ</td>\n",
       "      <td>2020-12-30 10:02:43.534</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>A+\\nLove it</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                      title  \\\n",
       "0       5  Such a lovely scent but not overpowering.   \n",
       "1       4     Works great but smells a little weird.   \n",
       "2       5                                       Yes!   \n",
       "3       1                          Synthetic feeling   \n",
       "4       5                                         A+   \n",
       "\n",
       "                                                text images        asin  \\\n",
       "0  This spray is really nice. It smells really go...     []  B00YQ6X8EO   \n",
       "1  This product does what I need it to do, I just...     []  B081TJ8YS3   \n",
       "2                          Smells good, feels great!     []  B07PNNCSP9   \n",
       "3                                     Felt synthetic     []  B09JS339BZ   \n",
       "4                                            Love it     []  B08BZ63GMJ   \n",
       "\n",
       "  parent_asin                       user_id               timestamp  \\\n",
       "0  B00YQ6X8EO  AGKHLEW2SOWHNMFQIJGBECAF7INQ 2020-05-05 14:08:48.923   \n",
       "1  B081TJ8YS3  AGKHLEW2SOWHNMFQIJGBECAF7INQ 2020-05-04 18:10:55.070   \n",
       "2  B097R46CSY  AE74DYR3QUGVPZJ3P7RFWBGIX7XQ 2020-05-16 21:41:06.052   \n",
       "3  B09JS339BZ  AFQLNQNQYFWQZPJQZS6V3NZU4QBQ 2022-01-28 18:13:50.220   \n",
       "4  B08BZ63GMJ  AFQLNQNQYFWQZPJQZS6V3NZU4QBQ 2020-12-30 10:02:43.534   \n",
       "\n",
       "   helpful_vote  verified_purchase  \\\n",
       "0             0               True   \n",
       "1             1               True   \n",
       "2             2               True   \n",
       "3             0               True   \n",
       "4             0               True   \n",
       "\n",
       "                                 concatenated_review  verified_purchase_int  \\\n",
       "0  Such a lovely scent but not overpowering.\\nThi...                      1   \n",
       "1  Works great but smells a little weird.\\nThis p...                      1   \n",
       "2                    Yes!\\nSmells good, feels great!                      1   \n",
       "3                  Synthetic feeling\\nFelt synthetic                      1   \n",
       "4                                        A+\\nLove it                      1   \n",
       "\n",
       "  sentiment  sentiment_negative  sentiment_neutral  sentiment_positive  \n",
       "0  positive                   0                  0                   1  \n",
       "1  positive                   0                  0                   1  \n",
       "2  positive                   0                  0                   1  \n",
       "3  negative                   1                  0                   0  \n",
       "4  positive                   0                  0                   1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper function to use ratings to rate the sentiment of reviews as our true values\n",
    "def create_review_sentiment(r):\n",
    "    # Treat non-numeric or missing as None\n",
    "    r_num = pd.to_numeric(r, errors=\"coerce\")\n",
    "    if pd.isna(r_num):\n",
    "        return None\n",
    "    if r_num <= 2:\n",
    "        return \"negative\"\n",
    "    if r_num == 3:\n",
    "        return \"neutral\"\n",
    "    return \"positive\"\n",
    "\n",
    "# Create the labels for sentiments based on the ratings \n",
    "amazon_df[\"sentiment\"] = amazon_df[\"rating\"].apply(create_review_sentiment)\n",
    "amazon_df[\"sentiment\"] = amazon_df[\"sentiment\"].astype(\"string\")\n",
    "\n",
    "# One Hot Encode the values into 3 int cols and add ot originak\n",
    "sentiment_one_hot = pd.get_dummies(amazon_df[\"sentiment\"], prefix=\"sentiment\", dtype=int)\n",
    "amazon_df = pd.concat([amazon_df, sentiment_one_hot], axis=1)\n",
    "amazon_df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24db248c",
   "metadata": {},
   "source": [
    "## 4 Text Sentiment Analysis\n",
    "\n",
    "Classify the reviews into 3 sentiment categories:\n",
    "- positive\n",
    "- negative\n",
    "- neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50849824",
   "metadata": {},
   "source": [
    "### 4.1 Naives Bayes\n",
    "#### 4.1.1  Naives Bayes (Concatenate review title and body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bce5397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.878266075577666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.82      0.82     29023\n",
      "     neutral       0.86      0.07      0.14     11261\n",
      "    positive       0.89      0.99      0.94    100022\n",
      "\n",
      "    accuracy                           0.88    140306\n",
      "   macro avg       0.86      0.63      0.63    140306\n",
      "weighted avg       0.88      0.88      0.85    140306\n",
      "\n",
      "Confusion:\n",
      " [[23803    65  5155]\n",
      " [ 3520   839  6902]\n",
      " [ 1361    77 98584]]\n"
     ]
    }
   ],
   "source": [
    "# Version 1 (Concatenation) - Declare the dependent and independent variables\n",
    "x = amazon_df[\"concatenated_review\"]\n",
    "y = amazon_df[\"sentiment\"]\n",
    "\n",
    "# 1. Build a TD-IDF vectorizer with specific vocab pruning and phrase length settings\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1, 2), max_df=0.95, min_df=3)\n",
    "\n",
    "# 2. Fit vectorizers on training data and transform to sparse matrices\n",
    "x_tfid = tfidf_vec.fit_transform(x)\n",
    "\n",
    "# 3. Create labels for the target var (0, 1, 2) e.g. for the 3 classes\n",
    "y = amazon_df[\"sentiment\"].to_numpy()\n",
    "\n",
    "# 4. Split the dataset to train a Naive Bayes Model\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_tfid, y, test_size=0.2, random_state=100, stratify=y)\n",
    "nb_model_1 = MultinomialNB(alpha=0.5)\n",
    "nb_model_1.fit(x_train, y_train)\n",
    "\n",
    "y_pred = nb_model_1.predict(x_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1747d2d",
   "metadata": {},
   "source": [
    "#### 4.1.2  Naives Bayes (Without concatenate review title and body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7b1b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2 (Without Concatenation)\n",
    "x = amazon_df[[\"title\", \"text\"]]\n",
    "y = amazon_df[\"sentiment\"]\n",
    "\n",
    "# 1. Prepare raw text inputs from the DataFrame\n",
    "titles = amazon_df[\"title\"].fillna(\"\").astype(str).tolist()   # list of title strings\n",
    "texts  = amazon_df[\"text\"].fillna(\"\").astype(str).tolist()    # list of body strings\n",
    "\n",
    "# 2. Build a TD-IDF vectorizer for the title and body, based on their characteristics\n",
    "title_tfidf_vec = TfidfVectorizer(ngram_range=(1, 2), max_df=0.90, min_df=2)\n",
    "text_tfidf_vec = TfidfVectorizer(ngram_range=(1, 2), max_df=0.95, min_df=3)\n",
    "\n",
    "# 3. Fit vectorizers on training data and transform to sparse matrices\n",
    "x_title_tfid = title_tfidf_vec.fit_transform(titles) # shape: (n_samples, n_title_features)\n",
    "x_text_tfid = text_tfidf_vec.fit_transform(texts)    # shape: (n_samples, n_text_features)\n",
    "\n",
    "# Concatenate the two feature blocks horizontally to form one design matrix\n",
    "# hstack([]) stacks two already-vectorized feature matrices side by side, keeping separate vocabularies and IDF statistics per field, then forms one big matrix for the model.\n",
    "x = hstack([x_title_tfid, x_text_tfid], format=\"csr\")  # shape: (n_samples, n_title + n_text)\n",
    "\n",
    "# 4. Create labels for the target var (0, 1, 2) e.g. for the 3 classes\n",
    "y = amazon_df[\"sentiment\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e24297dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89790885635682\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.90      0.85     29023\n",
      "     neutral       0.69      0.22      0.34     11261\n",
      "    positive       0.93      0.97      0.95    100022\n",
      "\n",
      "    accuracy                           0.90    140306\n",
      "   macro avg       0.81      0.70      0.71    140306\n",
      "weighted avg       0.89      0.90      0.88    140306\n",
      "\n",
      "Confusion:\n",
      " [[26158   438  2427]\n",
      " [ 4381  2510  4370]\n",
      " [ 1994   714 97314]]\n"
     ]
    }
   ],
   "source": [
    "# 5. Split the dataset to train a Naive Bayes Model\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=100, stratify=y)\n",
    "nb_model_2 = MultinomialNB(alpha=0.5)\n",
    "nb_model_2.fit(x_train, y_train)\n",
    "\n",
    "y_pred = nb_model_2.predict(x_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf9f0eb",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "- Naives Bayes is relatively accurate in text sentiment analysis, achieving near 90% of the accuracy\n",
    "- Not concatenating the reviews helps improve the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af53d684",
   "metadata": {},
   "source": [
    "### 4.2 BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff4bb4c",
   "metadata": {},
   "source": [
    "#### 4.2.1 BERT (Concatenate review and body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77bf79e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is too large at 700k reviews, hence we will only make use of a small subset of it\n",
    "# Helper function to pick subsets easily\n",
    "def get_subsets(train_tokens, test_tokens, size=\"small\"):\n",
    "    if size == \"small\":\n",
    "        train = train_tokens.shuffle(seed=100).select(range(5000))\n",
    "        test = test_tokens.shuffle(seed=100).select(range(500))\n",
    "    elif size == \"medium\":\n",
    "        train = train_tokens.shuffle(seed=100).select(range(20000))\n",
    "        test = test_tokens.shuffle(seed=100).select(range(2000))\n",
    "    elif size == \"large\":\n",
    "        train = train_tokens.shuffle(seed=100).select(range(50000))\n",
    "        test = test_tokens.shuffle(seed=100).select(range(5000))\n",
    "    else:\n",
    "        raise ValueError(\"size must be one of: small, medium, large\")\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98492d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the Dependent Y and Independent X\n",
    "x = amazon_df[\"concatenated_review\"]\n",
    "y = amazon_df[\"sentiment\"]\n",
    "\n",
    "# Since sentiment (y) is a string, we need to cast it to ints to work with BERT\n",
    "sentiment_classes = sorted(y.unique())\n",
    "# Create a mapping so we can decode what each int means in both ways\n",
    "label_to_id_idx = {c: i for i, c in enumerate(sentiment_classes)}\n",
    "id_idx_to_label = {i: c for c, i in label_to_id_idx.items()}\n",
    "\n",
    "# Convert strings to an array of ints like [pos neg neutral] as int vals\n",
    "y = y.map(label_to_id_idx).to_numpy()\n",
    "x = x.to_numpy()\n",
    "\n",
    "# Split once\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=100, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88779b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train and test datasets for tuning the BERT model\n",
    "train_df = pd.DataFrame({\"text\": x_train, \"labels\": y_train})\n",
    "test_df = pd.DataFrame({\"text\": x_test, \"labels\": y_test})\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a41c3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 561222/561222 [00:26<00:00, 21382.52 examples/s]\n",
      "Map: 100%|██████████| 140306/140306 [00:07<00:00, 19164.62 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Use the DistilBERT Model which is smaller first\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenise the Independent X\n",
    "def tokenize(batch):\n",
    "    # 128 to make them smaller\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=128)\n",
    "\n",
    "train_tokens = train_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "test_tokens = test_dataset.map(tokenize,  batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a78180aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Train the Distil BERT Model to better predict reviews\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Load a Classification Head\n",
    "num_review_labels = len(sentiment_classes)\n",
    "distil_bert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_review_labels,\n",
    "    id2label=id_idx_to_label,\n",
    "    label2id=label_to_id_idx,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": metric_acc.compute(predictions=preds, references=labels)[\"accuracy\"]}\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"bert_cls_runs\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    logging_steps=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4074101c",
   "metadata": {},
   "source": [
    "##### 4.2.1.1 Small Dataset + 3 Epoch for Train & Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16ac1b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w1/mvky9f4d4s19c7tj7wqmqyx00000gn/T/ipykernel_39374/1218129957.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_1 = Trainer(\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='939' max='939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [939/939 06:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.371800</td>\n",
       "      <td>0.321312</td>\n",
       "      <td>0.886000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.241300</td>\n",
       "      <td>0.307102</td>\n",
       "      <td>0.886000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.173300</td>\n",
       "      <td>0.326431</td>\n",
       "      <td>0.890000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=939, training_loss=0.29260404371478943, metrics={'train_runtime': 425.8999, 'train_samples_per_second': 35.22, 'train_steps_per_second': 2.205, 'total_flos': 321948090688680.0, 'train_loss': 0.29260404371478943, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subset, test_subset = get_subsets(train_tokens, test_tokens, size=\"small\")\n",
    "\n",
    "# Small Token Dataset\n",
    "trainer_1 = Trainer(\n",
    "    model=distil_bert_model,\n",
    "    args=args,\n",
    "    train_dataset=train_subset,\n",
    "    eval_dataset=test_subset    ,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer_1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "738e3eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.86      0.84       104\n",
      "     neutral       0.49      0.40      0.44        45\n",
      "    positive       0.95      0.96      0.96       351\n",
      "\n",
      "    accuracy                           0.89       500\n",
      "   macro avg       0.76      0.74      0.75       500\n",
      "weighted avg       0.88      0.89      0.89       500\n",
      "\n",
      "Confusion:\n",
      " [[ 89  10   5]\n",
      " [ 14  18  13]\n",
      " [  4   9 338]]\n"
     ]
    }
   ],
   "source": [
    "# Predictions on the same subset\n",
    "pred = trainer_1.predict(test_subset)\n",
    "y_pred = pred.predictions.argmax(-1)\n",
    "y_true = pred.label_ids\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred, target_names=[id_idx_to_label[i] for i in sorted(id_idx_to_label)]))\n",
    "print(\"Confusion:\\n\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c08a91d",
   "metadata": {},
   "source": [
    "##### 4.2.1.2 Small Dataset + 6 Epoch for Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae75c9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w1/mvky9f4d4s19c7tj7wqmqyx00000gn/T/ipykernel_39374/1901208193.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_2 = Trainer(\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1878' max='1878' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1878/1878 12:50, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.192200</td>\n",
       "      <td>0.425382</td>\n",
       "      <td>0.884000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.117200</td>\n",
       "      <td>0.519413</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.111900</td>\n",
       "      <td>0.533946</td>\n",
       "      <td>0.878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.582045</td>\n",
       "      <td>0.888000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.625233</td>\n",
       "      <td>0.884000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.632011</td>\n",
       "      <td>0.882000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1878, training_loss=0.08259289231671058, metrics={'train_runtime': 772.138, 'train_samples_per_second': 38.853, 'train_steps_per_second': 2.432, 'total_flos': 642797096328864.0, 'train_loss': 0.08259289231671058, 'epoch': 6.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"bert_cls_runs\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "train_subset, test_subset = get_subsets(train_tokens, test_tokens, size=\"small\")\n",
    "\n",
    "# Small Token Dataset\n",
    "trainer_2 = Trainer(\n",
    "    model=distil_bert_model,\n",
    "    args=args,\n",
    "    train_dataset=train_subset,\n",
    "    eval_dataset=test_subset    ,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer_2.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7753b6d3",
   "metadata": {},
   "source": [
    "**Conclusions about the Number of Epochs:**\n",
    "- For relatively small datasets, we can conclude that 3 epochs is ideal for training our model. This is because the accuracy stops increasing, and even falls afterwards.\n",
    "- However, we can see that validation loss did not decrease as well as shown above. This is a sign of overfitting which indicates that our training dataset used might be too small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067c3908",
   "metadata": {},
   "source": [
    "##### 4.2.1.3 Medium Dataset + 3 Epoch for Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d50202a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w1/mvky9f4d4s19c7tj7wqmqyx00000gn/T/ipykernel_39374/2813374169.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_3 = Trainer(\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 37:26, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.244500</td>\n",
       "      <td>0.261083</td>\n",
       "      <td>0.901500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.158800</td>\n",
       "      <td>0.359224</td>\n",
       "      <td>0.903500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.145400</td>\n",
       "      <td>0.392411</td>\n",
       "      <td>0.906500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.083400</td>\n",
       "      <td>0.457662</td>\n",
       "      <td>0.902500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5000, training_loss=0.1611403627872467, metrics={'train_runtime': 2247.3735, 'train_samples_per_second': 35.597, 'train_steps_per_second': 2.225, 'total_flos': 1701684816794136.0, 'train_loss': 0.1611403627872467, 'epoch': 4.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"bert_cls_runs\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "# Medium Token Dataset\n",
    "train_subset, test_subset = get_subsets(train_tokens, test_tokens, size=\"medium\")\n",
    "\n",
    "trainer_3 = Trainer(\n",
    "    model=distil_bert_model,\n",
    "    args=args,\n",
    "    train_dataset=train_subset,\n",
    "    eval_dataset=test_subset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer_3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18009403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9065\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.88      0.86       407\n",
      "     neutral       0.58      0.46      0.51       169\n",
      "    positive       0.96      0.97      0.96      1424\n",
      "\n",
      "    accuracy                           0.91      2000\n",
      "   macro avg       0.79      0.77      0.78      2000\n",
      "weighted avg       0.90      0.91      0.90      2000\n",
      "\n",
      "Confusion:\n",
      " [[ 358   28   21]\n",
      " [  50   77   42]\n",
      " [  19   27 1378]]\n"
     ]
    }
   ],
   "source": [
    "# Predictions on the same subset\n",
    "pred = trainer_3.predict(test_subset)\n",
    "y_pred = pred.predictions.argmax(-1)\n",
    "y_true = pred.label_ids\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred, target_names=[id_idx_to_label[i] for i in sorted(id_idx_to_label)]))\n",
    "print(\"Confusion:\\n\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581ef508",
   "metadata": {},
   "source": [
    "##### 4.2.1.4 Large Dataset + 3 Epoch for Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c950db09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w1/mvky9f4d4s19c7tj7wqmqyx00000gn/T/ipykernel_39374/2259511632.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_4 = Trainer(\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9375' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9375/9375 1:18:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.221900</td>\n",
       "      <td>0.265905</td>\n",
       "      <td>0.906600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.160600</td>\n",
       "      <td>0.325083</td>\n",
       "      <td>0.909600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.071800</td>\n",
       "      <td>0.394497</td>\n",
       "      <td>0.907000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9375, training_loss=0.17127845222473145, metrics={'train_runtime': 4709.2914, 'train_samples_per_second': 31.852, 'train_steps_per_second': 1.991, 'total_flos': 3180078397422216.0, 'train_loss': 0.17127845222473145, 'epoch': 3.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"bert_cls_runs\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "# Large Token Dataset\n",
    "train_subset, test_subset = get_subsets(train_tokens, test_tokens, size=\"large\")\n",
    "\n",
    "trainer_4 = Trainer(\n",
    "    model=distil_bert_model,\n",
    "    args=args,\n",
    "    train_dataset=train_subset,\n",
    "    eval_dataset=test_subset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer_4.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38675a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9096\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.89      0.87      1047\n",
      "     neutral       0.56      0.49      0.52       403\n",
      "    positive       0.96      0.96      0.96      3550\n",
      "\n",
      "    accuracy                           0.91      5000\n",
      "   macro avg       0.79      0.78      0.78      5000\n",
      "weighted avg       0.91      0.91      0.91      5000\n",
      "\n",
      "Confusion:\n",
      " [[ 929   75   43]\n",
      " [ 113  197   93]\n",
      " [  50   78 3422]]\n"
     ]
    }
   ],
   "source": [
    "# Predictions on the same subset\n",
    "pred = trainer_4.predict(test_subset)\n",
    "y_pred = pred.predictions.argmax(-1)\n",
    "y_true = pred.label_ids\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred, target_names=[id_idx_to_label[i] for i in sorted(id_idx_to_label)]))\n",
    "print(\"Confusion:\\n\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aa73d3",
   "metadata": {},
   "source": [
    "Longer Epochs to see if it is better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96b95e5",
   "metadata": {},
   "source": [
    "#### 4.2.2 BERT without Concatenation\n",
    "##### 4.2.2.1 Small Dataset + 3 Epoch for Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0a1c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = amazon_df[[\"title\", \"text\"]]\n",
    "y = amazon_df[\"sentiment\"]\n",
    "\n",
    "sentiment_classes = sorted(y.unique())\n",
    "label_to_id_idx = {c: i for i, c in enumerate(sentiment_classes)}\n",
    "id_idx_to_label = {i: c for c, i in label_to_id_idx.items()}\n",
    "\n",
    "y = y.map(label_to_id_idx).to_numpy()\n",
    "\n",
    "# Extract arrays; ensure strings and no NaNs\n",
    "titles = x[\"title\"].fillna(\"\").astype(str).to_numpy()\n",
    "texts  = x[\"text\"].fillna(\"\").astype(str).to_numpy()\n",
    "\n",
    "# Single stratified split\n",
    "xtr_title, xte_title, xtr_text, xte_text, y_train, y_test = train_test_split(\n",
    "    titles, texts, y, test_size=0.2, random_state=100, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f609127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build HF datasets with two text columns and 'labels'\n",
    "train_df = pd.DataFrame({\"title\": xtr_title, \"text\": xtr_text, \"labels\": y_train})\n",
    "test_df  = pd.DataFrame({\"title\": xte_title, \"text\": xte_text, \"labels\": y_test})\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_dataset  = Dataset.from_pandas(test_df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1faf90c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 561222/561222 [00:20<00:00, 27377.08 examples/s]\n",
      "Map: 100%|██████████| 140306/140306 [00:06<00:00, 20701.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"title\"],\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "train_tokens = train_dataset.map(tokenize, batched=True, remove_columns=[\"title\",\"text\"])\n",
    "test_tokens  = test_dataset.map(tokenize,  batched=True, remove_columns=[\"title\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d64de34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "\n",
    "num_review_labels = len(sentiment_classes)\n",
    "distil_bert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_review_labels,\n",
    "    id2label=id_idx_to_label,\n",
    "    label2id=label_to_id_idx,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # Works for both tuple and EvalPrediction\n",
    "    logits = eval_pred[0] if isinstance(eval_pred, tuple) else eval_pred.predictions\n",
    "    labels = eval_pred[1] if isinstance(eval_pred, tuple) else eval_pred.label_ids\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": metric_acc.compute(predictions=preds, references=labels)[\"accuracy\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a58d251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w1/mvky9f4d4s19c7tj7wqmqyx00000gn/T/ipykernel_39374/849586880.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_5 = Trainer(\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='939' max='939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [939/939 07:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.354000</td>\n",
       "      <td>0.306957</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.232100</td>\n",
       "      <td>0.293226</td>\n",
       "      <td>0.896000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.157500</td>\n",
       "      <td>0.324147</td>\n",
       "      <td>0.898000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=939, training_loss=0.27718083012980016, metrics={'train_runtime': 461.2906, 'train_samples_per_second': 32.517, 'train_steps_per_second': 2.036, 'total_flos': 324881053991352.0, 'train_loss': 0.27718083012980016, 'epoch': 3.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"bert_cls_runs\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "train_subset, test_subset = get_subsets(train_tokens, test_tokens, size=\"small\")\n",
    "\n",
    "trainer_5 = Trainer(\n",
    "    model=distil_bert_model,\n",
    "    args=args,\n",
    "    train_dataset=train_subset,\n",
    "    eval_dataset=test_subset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_5.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee0818c",
   "metadata": {},
   "source": [
    "**Analysis:**\n",
    "\n",
    "- We note that when the number of epochs and model being used to train is held constant, not concatenating the review body and title leads to higher model accuracy, validating our results from using a Naives Bayes model.\n",
    "- Number of epochs used is sufficient since it shows that we minimised validation loss before it started to rise again.\n",
    "- Accuracy has also plateaued.\n",
    "- Final solution should use model trained from separate review title and body.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "caf15e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.898\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.88      0.87       104\n",
      "     neutral       0.50      0.44      0.47        45\n",
      "    positive       0.95      0.96      0.96       351\n",
      "\n",
      "    accuracy                           0.90       500\n",
      "   macro avg       0.77      0.76      0.77       500\n",
      "weighted avg       0.89      0.90      0.90       500\n",
      "\n",
      "Confusion:\n",
      " [[ 91   9   4]\n",
      " [ 12  20  13]\n",
      " [  2  11 338]]\n"
     ]
    }
   ],
   "source": [
    "pred = trainer_5.predict(test_subset)\n",
    "y_pred = pred.predictions.argmax(-1)\n",
    "y_true = pred.label_ids\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred, target_names=[id_idx_to_label[i] for i in sorted(id_idx_to_label)]))\n",
    "print(\"Confusion:\\n\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d204a",
   "metadata": {},
   "source": [
    "##### 4.2.2.2 Medium Dataset + 3 Epoch for Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f57bf02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w1/mvky9f4d4s19c7tj7wqmqyx00000gn/T/ipykernel_39374/4090311133.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_6 = Trainer(\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 30:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.253300</td>\n",
       "      <td>0.248621</td>\n",
       "      <td>0.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.311493</td>\n",
       "      <td>0.904500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.164200</td>\n",
       "      <td>0.353564</td>\n",
       "      <td>0.901500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3750, training_loss=0.2000052101135254, metrics={'train_runtime': 1860.0737, 'train_samples_per_second': 32.257, 'train_steps_per_second': 2.016, 'total_flos': 1287696115193976.0, 'train_loss': 0.2000052101135254, 'epoch': 3.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"bert_cls_runs\",\n",
    "    eval_strategy=\"epoch\",    # if the installed transformers wants evaluation_strategy, switch to that\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "train_subset, test_subset = get_subsets(train_tokens, test_tokens, size=\"medium\")\n",
    "\n",
    "trainer_6 = Trainer(\n",
    "    model=distil_bert_model,\n",
    "    args=args,\n",
    "    train_dataset=train_subset,\n",
    "    eval_dataset=test_subset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_6.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32905e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1/250 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.908\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.90      0.87       407\n",
      "     neutral       0.56      0.49      0.52       169\n",
      "    positive       0.96      0.96      0.96      1424\n",
      "\n",
      "    accuracy                           0.91      2000\n",
      "   macro avg       0.79      0.78      0.79      2000\n",
      "weighted avg       0.91      0.91      0.91      2000\n",
      "\n",
      "Confusion:\n",
      " [[ 368   22   17]\n",
      " [  52   82   35]\n",
      " [  16   42 1366]]\n"
     ]
    }
   ],
   "source": [
    "pred = trainer_6.predict(test_subset)\n",
    "y_pred = pred.predictions.argmax(-1)\n",
    "y_true = pred.label_ids\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred, target_names=[id_idx_to_label[i] for i in sorted(id_idx_to_label)]))\n",
    "print(\"Confusion:\\n\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9921c6",
   "metadata": {},
   "source": [
    "##### 4.2.2.3 Large Dataset + 3 Epoch for Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5419953e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w1/mvky9f4d4s19c7tj7wqmqyx00000gn/T/ipykernel_39374/3473663951.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_7 = Trainer(\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9375' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9375/9375 1:26:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.232800</td>\n",
       "      <td>0.242460</td>\n",
       "      <td>0.911000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.209600</td>\n",
       "      <td>0.264407</td>\n",
       "      <td>0.912400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.108600</td>\n",
       "      <td>0.331888</td>\n",
       "      <td>0.909400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9375, training_loss=0.18876554985046387, metrics={'train_runtime': 5178.2739, 'train_samples_per_second': 28.967, 'train_steps_per_second': 1.81, 'total_flos': 3209993795173464.0, 'train_loss': 0.18876554985046387, 'epoch': 3.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"bert_cls_runs\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "train_subset, test_subset = get_subsets(train_tokens, test_tokens, size=\"large\")\n",
    "\n",
    "trainer_7 = Trainer(\n",
    "    model=distil_bert_model,\n",
    "    args=args,\n",
    "    train_dataset=train_subset,\n",
    "    eval_dataset=test_subset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_7.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36820e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngxua/Documents/ComputerScience_Projects/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9124\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.90      0.87      1047\n",
      "     neutral       0.57      0.44      0.50       403\n",
      "    positive       0.96      0.97      0.97      3550\n",
      "\n",
      "    accuracy                           0.91      5000\n",
      "   macro avg       0.79      0.77      0.78      5000\n",
      "weighted avg       0.91      0.91      0.91      5000\n",
      "\n",
      "Confusion:\n",
      " [[ 942   72   33]\n",
      " [ 131  179   93]\n",
      " [  45   64 3441]]\n"
     ]
    }
   ],
   "source": [
    "pred = trainer_7.predict(test_subset)\n",
    "y_pred = pred.predictions.argmax(-1)\n",
    "y_true = pred.label_ids\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred, target_names=[id_idx_to_label[i] for i in sorted(id_idx_to_label)]))\n",
    "print(\"Confusion:\\n\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8cc5ac",
   "metadata": {},
   "source": [
    "**Final Conclusions:**\n",
    "\n",
    "1. We note that the BERT model that was trained with the largest dataset has the highest accuracy at 91.2% out of all the models we trained. Hence, we would use it in our final review sentiment analysis solution.\n",
    "2. We also note that the Naives Bayes model used, being a simple classical machine learning model, could be trained more quickly and with the entire dataset on a laptop GPU, compared to a deep learning model like BERT. They were also able to achieve similar levels of accuracy, peaking at 89% approximately.\n",
    "3. Hence, in our final solution we can make use of the best model to use text sentiment analysis to determine if a review is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc396bc",
   "metadata": {},
   "source": [
    "# Storing the Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "130c54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"bert_sentiment_best\"\n",
    "\n",
    "trainer_7.save_model(save_dir)  # model weights + config; use this if you prefer Trainer API [web:207]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee5ac2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)                  # reload tokenizer [web:325]\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir) # reload model [web:325]\n",
    "clf = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)  # no top_k here\n",
    "\n",
    "def predict_sentiment(text: str):\n",
    "    res = clf(text)[0]\n",
    "    return f\"{res['label']} ({res['score']:.3f})\"\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=predict_sentiment,\n",
    "    inputs=gr.Textbox(lines=3, placeholder=\"Enter a review...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Retail Review Sentiment\",\n",
    "    description=\"Enter a product review to see the predicted sentiment.\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbcafc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
